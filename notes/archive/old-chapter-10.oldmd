Figure 8.2 Life Concepts > Figure 10.1
Figure 8.3 to 10.2 (TV watch history)

RENAME FILES Too


Notes from 10.2

<span class="editnote">ADD NEW SECTION ON PRAGMATIC REFLECTIONS. starting by saying what C8 did and didn't do in terms of obstacles etc.</span>
<span class="editnote">add some words highlighting that there are very difficult questions, but that by operationalising HDR i show some of what is possible, not an answer to these questions but a direction of travel. talk about viability and the need for these practical endeavours to acknowledge the challenges i identified. notes: Resolving this tension between what is wanted, what is possible (technically) and what barriers are organisationally/legally is interesting. The contribution here is showing what is possible and how to get there. Whilst I also like the optimism about how to leverage law and design to support and help citizens, still need for more criticality about viability of these routes in practice? How do we balance the paternalism with law, and values defined as societally beneficial e.g., transparency, accountability, privacy, the local desire for these to be realised, as shown in your data, and the reality of surveillance capitalism and ‘if you’re not paying, you’re the product’? </span>
<span class="editnote">write more about HDR literacy and *whose responsibility it is* (ref recursive public, policy, designers - all designers should consider this - etc). I need to more explicitly talk about the burden/impact on the citizen (as Dave said, use the accountant analogy, etc, or environmental impact advise, or mechanic, etc)). Notes: HDR literacy - again, how much of this should be pushed to individuals, e.g. to rely on their legal rights and seek portability/erasure etc? How much is this failure of system/service design where this should be done by default and implemented to prevent putting burden on users in first place? <br/> More notes from examiners: <br/> Usability point - what support for ‘what next’ does HDR provide – once individuals have oversight, what then? Is that enough, or is this also about challenging current business models? Whilst choice is good – it does push debate to what are they choosing in relation to? And do they have capacity, time, energy, means to really choose (like with consent?) Wary of pushing too much back to the citizen to deal with, when these are structural design decisions around service delivery or business model, and regulatory failings (often as a result of lobbying during legislative process).</span>
<span class="editnote">Section about Feasibility / Practicality? A Challenging Road Ahead / Designing the Future.</span>
<span class="editnote">more specific discussion on the challenges of unification and liabilities. make clearer the relationship between law and desired change, and how to balance needs of different parties.notes: Given the focus on GDPR, how much do you see law as enabling or inhibiting changes in design practice and balancing value conflict? e.g., process transparency around data processing vs use of IP rights to limit AI transparency? or individual oversight (which may be limited in its utility) vs collective oversight or enforcement authority ? . notes: The idea of unifying data is interesting as whilst it could add value from a usability perspective, giving an understanding of data insights (p243)– it prompts questions though around DP realities of linking data, and roles of different controllers/processors around managing their various obligations if it is unified e.g. risks of new identification harms/connections being made that otherwise were partitioned. </span>
<span class="editnote">legal vs reality difference: How do we align what people want with governance and sociotechnical structures that shape what they can have? For example, many of the requirements elicited from fieldwork broadly reflect what we would hope would be best practice from many legal frameworks (GDPR, ePrivacy Directive, Consumer Rights Act, DSA etc), yet power structures around services are such they are not realisable in practice? I am not sure what is actionable for me here in this c1 feedback point. Ask the examiners?</span>
<span class="editnote">highlight the design problem. The future work section should frame some of what i have offered in C6 as a design problem (informed by the insights). notes: What mechanisms are there to support individuals navigating their data in an everyday way and how can design respond to that complex problem? HDR seems well placed to contribute here not just defaulting to XAI (explainable AI) type approaches which may have little value for individuals?</span>
<span class="editnote">Notes on how we can apply HDR to design practice: There can be a clear disconnect between what law may frame as being protected vs what is possible in practice... How do we reconcile expectations/wants with reality of what is feasible/aspirational/implementable? What role can designers play – i.e., how can we apply HDR to design practice? This involves considering the different roles of HDR from activist adversarial orientated through to establishing areas where greater translation between design and law is needed to supporting compliance with GDPR. Good point from discussion around inadequacy of what GDPR seeks at times too e.g. data changing all the time – copy is not working – data access should be around person understanding of data... if bundle of files this is static and loses nature of data being interpretable and relational.</span>
<span class="editnote">consideration of e.g. pseudonymous data, CCTV, AI emotion identification, etc and how that relates to PD (I would argue, still needs to be made understandable, however hard). Notes: Attending to importance of specificity of law in defining the problem space for HDR. 2 examples. Personal data (PD) is a term of legal art - if there is not identification/singling out of individuals then it might not be PD, and therefore no rights under GDPR. This occurs with data driven systems where intent is not identification or unclear if PD being processed and therefore scope of protection limited... e.g. emotion sensing AI systems. How does your work deal with that tension in limits of the law, and also of what people may want or feel is there data may not in fact have the legal protections they desire around it?</span>
<span class="editnote">What if they circumvent Consent? I need to write somewhere about 'alternatives to consent' within chap 1 (indeed I reference it in TikTok example and SILVER). Notes: Lots of discussion around the role of consent (and issues of it being one time; severance model etc), but recall consent is only one of the legal grounds for data processing. What do you do when that is not the legal grounds for processing? How does this impact HDR? There are many others (fulfilling contract; public duty; legitimate interests). Thus, be careful that whilst consent may encourage a more relational dynamic (despite its current flaws), these other grounds could be used, and indeed could be seen as being designed in some ways to minimise input of subjects and to give a legal justification to processing for controllers, without need to engage with subjects.</span>

<span class="editnote">risk of people having to sacrifice data, no choice. (ref to C5?)</span>
<span class="editnote">individual data being lost in a big data whole, and how that doesn't absolve the company of responsibility, or the individual of having an interest (maybe use an example like car traffic data... or CCTV.. how could it be accountable / providence?). Notes: Related to this, with the uncertainty, opacity etc around how data is consumed/used in organisations (training data for models etc) - how does this implicate the model of human data relations? how do we have relations when the data may be a mere weighted variable in a larger decision making process? Does this lead to apathy for direct relations and concern around indirect relations and what does HDR offer in dealing with this?</span>
<span class="editnote">Notes on operationalising: Reframing that this is not PAR, due to lack of feedback loop from AR interventions. There is iterative process. Actually your approach can be justified as something novel in its own right due to your placements helping you understand how to operationalise HDR. Need to unpack the lessons around operationalising. </span>
